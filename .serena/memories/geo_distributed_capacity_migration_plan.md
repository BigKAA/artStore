# ĞŸĞ»Ğ°Ğ½ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¸: Geo-Distributed Capacity Management Ñ Leader Election

**Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½:** 2025-12-04
**ĞĞ±Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½:** 2025-12-04
**Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ:** Phase 2 Testing COMPLETE âœ…

## ğŸ¯ Ğ¦ĞµĞ»Ğ¸ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¸

1. ĞŸĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ Redis-based push Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº HTTP polling Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
2. ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° Storage Elements Ğ² ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¦ĞĞ” (Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Redis)
3. Ğ“Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ingester Ñ Leader Election
4. Ğ¡Ğ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞµÑ‚ĞµĞ²Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ñ„Ğ¸ĞºĞ° Ğ½Ğ° 75%

## ğŸ“‹ ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€ĞµÑˆĞ°ĞµĞ¼

### ĞŸĞµÑ€Ğ²Ğ¸Ñ‡Ğ½Ğ°Ñ: Storage Elements Ğ² ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¦ĞĞ”
- ĞĞµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Redis
- ĞĞµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· reverse proxy/PTF/WAF
- Ğ¢ĞµĞºÑƒÑ‰Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° (Storage â†’ Redis â† Ingester) Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚

### Ğ’Ñ‚Ğ¾Ñ€Ğ¸Ñ‡Ğ½Ğ°Ñ: Ğ“Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ingester
- Ğ‘ĞµĞ· coordination: 4 Ingester Ã— 100 SE = 1,152,000 requests/day
- Ğ”ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ polling, waste resources
- Ğ¡ Leader Election: 288,000 requests/day (75% reduction)

## ğŸ—ï¸ Ğ¦ĞµĞ»ĞµĞ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Control Plane â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  N Ã— Ingester (Leader Election via Redis)             â”‚
â”‚  - 1 LEADER: Polling Storage Elements                 â”‚
â”‚  - N-1 FOLLOWERS: Reading from shared Redis cache     â”‚
â”‚                                                         â”‚
â”‚  Redis (Shared):                                       â”‚
â”‚  - capacity_monitor:leader_lock (TTL=30s)             â”‚
â”‚  - capacity:{se_id} (TTL=600s)                        â”‚
â”‚  - health:{se_id} (TTL=600s)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Ğ¢ĞĞ›Ğ¬ĞšĞ Leader polls   â”‚
          â†“                        â†“
    Reverse Proxy / PTF / WAF
          â”‚
          â†“
    Storage Elements (DC1-5)
    GET /api/v1/capacity
```

## ğŸ“… Migration Phases

### Phase 1: Implementation (Sprint 17) âœ… COMPLETE

#### âœ… COMPLETED - Storage Element
- [x] `/api/v1/capacity` endpoint
- [x] `CapacityService` (Local FS + S3 support)
- [x] Configuration fields (datacenter_location, s3_soft_capacity_limit)
- [x] Router registration
- [x] Commit: `dc45fbf` merged to main

#### âœ… COMPLETED - Ingester Module
- [x] `AdaptiveCapacityMonitor` service Ñ Leader Election
- [x] Leader Election logic (Redis SET NX EX)
- [x] Leader/Follower modes Ñ automatic failover
- [x] HTTP polling Ñ exponential backoff (2s, 4s, 8s)
- [x] Redis cache Ğ´Ğ»Ñ capacity Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (TTL=600s)
- [x] Lazy update trigger Ğ´Ğ»Ñ 507 errors
- [x] `CapacityMonitorConfig` configuration class
- [x] `CapacityMonitorSettings` Ğ² config.py
- [x] Integration Ğ² main.py lifespan
- [x] Commit: `6149077` feat(ingester): Add AdaptiveCapacityMonitor with Redis Leader Election

#### âœ… COMPLETED - Prometheus Metrics
- [x] `capacity_monitor_leader_state` - Leader/Follower status
- [x] `capacity_monitor_leader_transitions_total` - acquired/lost/renewed
- [x] `leader_lock_acquisition_duration_seconds` - lock latency
- [x] `capacity_poll_duration_seconds` - polling latency per SE
- [x] `capacity_poll_failures_total` - failures by error type
- [x] `lazy_update_triggers_total` - lazy update events
- [x] `storage_elements_available` - available SE by mode
- [x] `capacity_cache_hits_total` - cache hit/miss rate

#### âœ… COMPLETED - UploadService Integration
- [x] `InsufficientStorageException` for HTTP 507 handling
- [x] `UploadService` retry logic (3 attempts max)
- [x] `excluded_se_ids` parameter in StorageSelector
- [x] Lazy update integration via `set_capacity_monitor()`
- [x] Health checks (Capacity Monitor status, edit-mode SE count)
- [x] Commit: `e97a765` feat(ingester): Add retry logic, lazy update, and capacity health checks

#### âœ… MERGED TO MAIN
- [x] Merge commit: `f0dddde` Merge branch 'feature/ingester-adaptive-capacity-monitor'
- [x] Feature branch deleted

### Phase 2: Testing (Sprint 17, Week 2) âœ… COMPLETE

#### âœ… COMPLETED - Unit Tests
- [x] **AdaptiveCapacityMonitor Unit Tests** (44 tests)
  - StorageCapacityInfo serialization (3 tests)
  - Leader Election logic (10 tests)
  - Capacity Polling (7 tests)
  - Cache Operations (5 tests)
  - Lazy Update mechanism (3 tests)
  - Monitor Lifecycle (5 tests)
  - Status & Metrics (3 tests)
  - Global Singleton (2 tests)
  - Adaptive Polling State (2 tests)
  - **File:** `ingester-module/tests/unit/test_capacity_monitor.py`

- [x] **CapacityService Unit Tests** (19 tests)
  - Local filesystem capacity (6 tests)
  - S3 capacity calculation (5 tests)
  - get_capacity_info dispatcher (5 tests)
  - FastAPI dependency (1 test)
  - Precision calculations (2 tests)
  - **File:** `storage-element/tests/unit/test_capacity_service.py`

#### âœ… COMPLETED - Integration Tests
- [x] **Leader Election Failover** (12 tests)
  - Single instance becomes Leader
  - Second instance becomes Follower
  - Follower promotion after TTL expiry
  - Leader renews lock periodically
  - Graceful leadership release
  - Rapid Leader succession
  - Brief Redis hiccup tolerance
  - Cache survives Leader change
  - Lazy update for Follower
  - Concurrent access (only one Leader)
  - **File:** `ingester-module/tests/integration/test_capacity_monitor_failover.py`

- [x] **Adaptive Polling** (13 tests)
  - Initial interval is base_interval
  - Failure count increments on poll failure
  - Success resets failure count
  - Leader executes polling loop
  - Multiple Storage Elements polled
  - Polling updates Redis cache
  - get_capacity returns cached data
  - HTTP timeout handling
  - HTTP error response handling
  - Parallel polling multiple SE
  - Follower reads from cache
  - Status includes polling info
  - Health status reflects polling state
  - **File:** `ingester-module/tests/integration/test_adaptive_polling.py`

#### ğŸ“Š Test Summary
| Module | Test Type | Tests | Status |
|--------|-----------|-------|--------|
| Ingester | Unit (Capacity Monitor) | 44 | âœ… PASSED |
| Storage Element | Unit (CapacityService) | 19 | âœ… PASSED |
| Ingester | Integration (Failover) | 12 | âœ… PASSED |
| Ingester | Integration (Polling) | 13 | âœ… PASSED |
| **TOTAL** | | **88** | âœ… **ALL PASSED** |

### Phase 3: Parallel Run (Sprint 18)

- [ ] Deploy AdaptiveCapacityMonitor (parallel Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Redis write Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹)
- [ ] Storage Elements Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ÑÑ‚ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ² Redis (compatibility)
- [ ] Monitoring: Leader transitions, poll metrics, cache consistency
- [ ] Validation: ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²
- [ ] Duration: 1 week minimum

### Phase 4: Cutover (Sprint 19)

- [ ] Verify Leader Election stability (>99.9% uptime)
- [ ] Ingester Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ¸Ğ· capacity cache
- [ ] Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Redis write logic Ğ¸Ğ· Storage Elements
- [ ] Cleanup ÑÑ‚Ğ°Ñ€Ñ‹Ñ… Redis keys
- [ ] Full production monitoring
- [ ] Rollback plan validation

## ğŸ”§ Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸

### Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ (Phase 1 + Phase 2)

```
storage-element/
â”œâ”€â”€ app/api/v1/endpoints/capacity.py      # NEW - /capacity endpoint
â”œâ”€â”€ app/services/capacity_service.py      # NEW - CapacityService
â”œâ”€â”€ app/api/v1/router.py                  # MODIFIED - router registration
â”œâ”€â”€ app/core/config.py                    # MODIFIED - datacenter_location, s3_soft_limit
â””â”€â”€ tests/unit/test_capacity_service.py   # NEW - 19 unit tests

ingester-module/
â”œâ”€â”€ app/services/capacity_monitor.py      # NEW - AdaptiveCapacityMonitor (~1000 lines)
â”œâ”€â”€ app/core/config.py                    # MODIFIED - CapacityMonitorSettings
â”œâ”€â”€ app/core/metrics.py                   # MODIFIED - Leader Election metrics (8 new)
â”œâ”€â”€ app/core/exceptions.py                # MODIFIED - InsufficientStorageException
â”œâ”€â”€ app/services/upload_service.py        # MODIFIED - retry logic, lazy update
â”œâ”€â”€ app/services/storage_selector.py      # MODIFIED - excluded_se_ids support
â”œâ”€â”€ app/api/v1/endpoints/health.py        # MODIFIED - capacity monitor health checks
â”œâ”€â”€ app/main.py                           # MODIFIED - lifespan integration
â”œâ”€â”€ tests/unit/test_capacity_monitor.py   # NEW - 44 unit tests
â”œâ”€â”€ tests/integration/test_capacity_monitor_failover.py  # NEW - 12 integration tests
â””â”€â”€ tests/integration/test_adaptive_polling.py           # NEW - 13 integration tests
```

### Redis Cache Structure

```redis
# Leader Election
capacity_monitor:leader_lock = "ingester-instance-uuid"
TTL: 30s

# Capacity Data (Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ÑÑ Leader)
capacity:{se_id} = {
  "storage_id": "se-dc2-01",
  "mode": "rw",
  "total": "...",
  "used": "...",
  "available": "...",
  "percent_used": "...",
  "health": "healthy",
  "backend": "local",
  "location": "dc2",
  "last_update": "ISO8601",
  "last_poll": "ISO8601",
  "endpoint": "https://..."
}
TTL: 600s

# Health Status
health:{se_id} = "healthy" | "unhealthy: <reason>"
TTL: 600s
```

### Leader Election Logic

```python
# Atomic leadership acquisition
acquired = await redis.set(
    "capacity_monitor:leader_lock",
    instance_id,
    nx=True,  # SET only if NOT exists
    ex=30,    # Expire after 30s
)

# Leadership renewal (Leader only)
if is_leader:
    await redis.expire("capacity_monitor:leader_lock", 30)
```

### Retry Logic with Lazy Update

```python
# UploadService retry pattern
excluded_se_ids = set()
for attempt in range(max_retries):
    se = await storage_selector.select_storage_element(
        file_size=size,
        excluded_se_ids=excluded_se_ids
    )
    try:
        return await _upload_to_storage_element(se, ...)
    except InsufficientStorageException as e:
        excluded_se_ids.add(e.storage_element_id)
        if capacity_monitor:
            await capacity_monitor.trigger_lazy_update(e.storage_element_id)
```

### Automatic Failover Timeline

```
T=0s:   Ingester-01 LEADER (TTL=30s)
T=15s:  Ingester-01 crashes
T=30s:  Lock expires
T=31s:  Ingester-02 acquires lock â†’ becomes LEADER
        
Max failover time: 30s
Cache remains valid: 600s (TTL)
```

### Configuration Parameters

```bash
# Storage Element
STORAGE_ELEMENT_ID=se-dc2-01
STORAGE_DATACENTER_LOCATION=dc2
STORAGE_TYPE=local|s3
STORAGE_EXTERNAL_ENDPOINT=https://se-dc2-01.example.com
STORAGE_S3_SOFT_CAPACITY_LIMIT=10995116277760  # 10TB

# Ingester (NEW - Sprint 17)
CAPACITY_MONITOR_ENABLED=on
CAPACITY_MONITOR_LEADER_TTL=30
CAPACITY_MONITOR_LEADER_RENEWAL_INTERVAL=10
CAPACITY_MONITOR_BASE_INTERVAL=30
CAPACITY_MONITOR_MAX_INTERVAL=300
CAPACITY_MONITOR_HTTP_TIMEOUT=15
CAPACITY_MONITOR_HTTP_RETRIES=3
CAPACITY_MONITOR_RETRY_BASE_DELAY=2.0
CAPACITY_MONITOR_CACHE_TTL=600
CAPACITY_MONITOR_HEALTH_TTL=600
CAPACITY_MONITOR_FAILURE_THRESHOLD=3
CAPACITY_MONITOR_RECOVERY_THRESHOLD=2
```

## ğŸ“Š Prometheus Metrics

### Leader Election
- `capacity_monitor_leader_state{instance_id}` - 1=leader, 0=follower
- `capacity_monitor_leader_transitions_total{instance_id, transition_type}` - acquired/lost/renewed
- `leader_lock_acquisition_duration_seconds{result}` - latency

### Capacity Polling
- `capacity_poll_duration_seconds{storage_id, status}` - poll latency
- `capacity_poll_failures_total{storage_id, error_type}` - failures
- `lazy_update_triggers_total{storage_id, reason}` - stale cache events
- `storage_elements_available{mode}` - available SE count
- `capacity_cache_hits_total{result}` - cache hit/miss

## âš ï¸ Known Limitations

1. **Leader Failover Window:** Max 30s Ğ±ĞµĞ· polling
   - Mitigation: Cache TTL=600s, lazy update

2. **Redis Dependency:** Redis down = no Leader Election
   - Mitigation: Redis HA (Sentinel), followers use stale cache

3. **Eventual Consistency:** 30s-5min capacity staleness
   - Mitigation: Lazy update Ğ½Ğ° 507 errors

## ğŸ”„ Rollback Plan

### Phase 3 Rollback (Parallel Run)
1. Stop AdaptiveCapacityMonitor Ğ½Ğ° Ğ²ÑĞµÑ… Ingester
2. Storage Elements Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ÑÑ‚ Redis write (unchanged)
3. Ingester Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ Ğ¸Ğ· ÑÑ‚Ğ°Ñ€Ğ¾Ğ³Ğ¾ Redis source
4. Zero downtime rollback

### Phase 4 Rollback (After Cutover)
1. Re-enable Redis write Ğ½Ğ° Storage Elements
2. Restart Storage Elements
3. Switch Ingester back to old Redis source
4. Stop AdaptiveCapacityMonitor
5. Expected downtime: 5-10 minutes

## ğŸ“š Ğ¡ÑÑ‹Ğ»ĞºĞ¸

- **Documentation:** `claudedocs/geo-distributed-capacity-management-solution.md`
- **Storage Element endpoint:** `storage-element/app/api/v1/endpoints/capacity.py`
- **Capacity Service:** `storage-element/app/services/capacity_service.py`
- **Capacity Monitor:** `ingester-module/app/services/capacity_monitor.py`
- **Configuration:** `ingester-module/app/core/config.py`
- **Metrics:** `ingester-module/app/core/metrics.py`
- **Exceptions:** `ingester-module/app/core/exceptions.py`
- **Upload Service:** `ingester-module/app/services/upload_service.py`
- **Health Checks:** `ingester-module/app/api/v1/endpoints/health.py`
- **Unit Tests (Monitor):** `ingester-module/tests/unit/test_capacity_monitor.py`
- **Unit Tests (Service):** `storage-element/tests/unit/test_capacity_service.py`
- **Integration Tests (Failover):** `ingester-module/tests/integration/test_capacity_monitor_failover.py`
- **Integration Tests (Polling):** `ingester-module/tests/integration/test_adaptive_polling.py`

## âœ… Success Criteria

### Phase 1 (Implementation) âœ… ACHIEVED
- [x] Storage Element /capacity endpoint working
- [x] AdaptiveCapacityMonitor with Leader Election implemented
- [x] All Prometheus metrics defined
- [x] Configuration classes created
- [x] Integration in main.py lifespan
- [x] UploadService retry logic with excluded_se_ids
- [x] Lazy update integration
- [x] Health checks for capacity monitor

### Phase 2 (Testing) âœ… ACHIEVED
- [x] 88 tests written and passing
- [x] Unit test coverage for Leader Election, Polling, Cache, Lazy Update
- [x] Unit test coverage for CapacityService (Local FS + S3)
- [x] Integration tests for failover scenarios (12 tests)
- [x] Integration tests for adaptive polling (13 tests)
- [x] Failover time validated in tests

### Phase 3 (Parallel Run)
- [ ] Leader Election uptime > 99.9%
- [ ] Cache consistency > 99%
- [ ] No impact on upload latency
- [ ] Traffic reduction visible in metrics

### Phase 4 (Cutover)
- [ ] Zero downtime migration
- [ ] 75% traffic reduction confirmed
- [ ] All alerts configured and tested
- [ ] Runbook documented and validated

## ğŸ“ Git Commits

1. `dc45fbf` - feat(storage-element): Add /capacity endpoint for geo-distributed polling
2. `6149077` - feat(ingester): Add AdaptiveCapacityMonitor with Redis Leader Election
3. `e97a765` - feat(ingester): Add retry logic, lazy update, and capacity health checks
4. `f0dddde` - Merge branch 'feature/ingester-adaptive-capacity-monitor'
5. _(pending)_ - test: Add comprehensive unit and integration tests for capacity monitoring
